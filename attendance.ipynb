{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ac292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 23:28:03.259536: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-05 23:28:03.259803: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-05 23:28:03.344387: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-05 23:28:07.910301: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-05 23:28:07.911851: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imutils import paths\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    import face_recognition\n",
    "except Exception as e:\n",
    "    raise ImportError(\"face_recognition library is required. Install with: pip install face_recognition\")\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from keras import layers, models\n",
    "except Exception as e:\n",
    "    tf = None\n",
    "\n",
    "DATA_DIR = \"dataset_faces\"\n",
    "EMOTION_MODEL_PATH = \"models/emotion_model.keras\"\n",
    "FACE_ENCODER_PATH = \"models/face_encoder.pkl\"\n",
    "FACE_CLASSIFIER_PATH = \"models/face_classifier.pkl\"\n",
    "ATTENDANCE_CSV = \"datasets/attendance.csv\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63fe94ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting capture for 'alice'. Press 'q' to stop early.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured 60 images for alice at dataset_faces/alice\n"
     ]
    }
   ],
   "source": [
    "def collect_images(name, num_images=60, delay=0.2):\n",
    "    \"\"\"Capture images of a student from webcam and save to dataset directory.\"\"\"\n",
    "    person_dir = os.path.join(DATA_DIR, name)\n",
    "    os.makedirs(person_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(f\"Starting capture for '{name}'. Press 'q' to stop early.\")\n",
    "    count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        cv2.putText(frame, f\"Capture: {count}/{num_images}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "        cv2.imshow(\"Collect Images - Press q to quit\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "        if count < num_images:\n",
    "            filename = os.path.join(person_dir, f\"{name}_{int(time.time()*1000)}_{count}.jpg\")\n",
    "            cv2.imwrite(filename, frame)\n",
    "            count += 1\n",
    "            time.sleep(delay)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Captured {count} images for {name} at {person_dir}\")\n",
    "\n",
    "collect_images(\"alice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d418882b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images and computing embeddings. This may take a while...\n",
      "Face classifier trained and saved:\n",
      " - models/face_encoder.pkl\n",
      " - models/face_classifier.pkl\n"
     ]
    }
   ],
   "source": [
    "def train_face_classifier():\n",
    "    \"\"\"Compute face embeddings using face_recognition and train an SVM classifier.\"\"\"\n",
    "    print(\"Loading images and computing embeddings. This may take a while...\")\n",
    "    imagePaths = list(paths.list_images(DATA_DIR))\n",
    "    knownEncodings = []\n",
    "    knownNames = []\n",
    "\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "        image = cv2.imread(imagePath)\n",
    "        if image is None:\n",
    "            continue\n",
    "        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        boxes = face_recognition.face_locations(rgb, model='hog')\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "        if len(encodings) == 0:\n",
    "            continue\n",
    "        knownEncodings.append(encodings[0])\n",
    "        knownNames.append(name)\n",
    "\n",
    "    if len(knownEncodings) == 0:\n",
    "        print(\"No face encodings found. Make sure dataset is collected and faces are visible.\")\n",
    "        return\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(knownNames)\n",
    "\n",
    "    clf = SVC(C=1.0, kernel='linear', probability=True)\n",
    "    clf.fit(knownEncodings, y)\n",
    "\n",
    "    # save encoder and classifier\n",
    "    with open(FACE_ENCODER_PATH, 'wb') as f:\n",
    "        pickle.dump(le, f)\n",
    "    with open(FACE_CLASSIFIER_PATH, 'wb') as f:\n",
    "        pickle.dump(clf, f)\n",
    "\n",
    "    print(\"Face classifier trained and saved:\")\n",
    "    print(\" -\", FACE_ENCODER_PATH)\n",
    "    print(\" -\", FACE_CLASSIFIER_PATH)\n",
    "\n",
    "train_face_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95905143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_emotion_model(input_shape=(48,48,1), n_classes=7):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2,2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(n_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b99e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fer_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    X = []\n",
    "    y = []\n",
    "    for idx, row in df.iterrows():\n",
    "        pixels = row['pixels'].split()\n",
    "        img = np.asarray(pixels, dtype='uint8').reshape(48,48)\n",
    "        X.append(img)\n",
    "        y.append(row['emotion'])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X = X.astype('float32') / 255.0\n",
    "    X = np.expand_dims(X, -1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49f56b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_emotion_model(csv_path, epochs=20, batch_size=64):\n",
    "    if tf is None:\n",
    "        print(\"TensorFlow not available. Install with: pip install tensorflow\")\n",
    "        return\n",
    "    print(\"Loading FER dataset from:\", csv_path)\n",
    "    X, y = load_fer_csv(csv_path)\n",
    "    split = int(0.8 * len(X))\n",
    "    X_train, X_val = X[:split], X[split:]\n",
    "    y_train, y_val = y[:split], y[split:]\n",
    "\n",
    "    model = build_emotion_model(input_shape=X.shape[1:], n_classes=len(np.unique(y)))\n",
    "    model.summary()\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size)\n",
    "    model.save(EMOTION_MODEL_PATH)\n",
    "    print(\"Emotion model trained and saved to\", EMOTION_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62bd80a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757095357.097937   22114 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1757095357.108356   22114 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded emotion model.\n",
      "Current time 23:32:37.976382. Outside attendance window 09:30:00 - 10:30:00.\n"
     ]
    }
   ],
   "source": [
    "def recognize_and_mark_attendance(time_window=(datetime.time(9,30), datetime.time(10,30))):\n",
    "    if not os.path.exists(FACE_ENCODER_PATH) or not os.path.exists(FACE_CLASSIFIER_PATH):\n",
    "        print(\"Face classifier not found. Run 'train_face' first.\")\n",
    "        return\n",
    "\n",
    "    with open(FACE_ENCODER_PATH, 'rb') as f:\n",
    "        le = pickle.load(f)\n",
    "    with open(FACE_CLASSIFIER_PATH, 'rb') as f:\n",
    "        clf = pickle.load(f)\n",
    "\n",
    "    emotion_model = None\n",
    "    if os.path.exists(EMOTION_MODEL_PATH) and tf is not None:\n",
    "        try:\n",
    "            emotion_model = models.load_model(EMOTION_MODEL_PATH)\n",
    "            print(\"Loaded emotion model.\")\n",
    "        except Exception as e:\n",
    "            print(\"Could not load emotion model:\", e)\n",
    "            emotion_model = None\n",
    "\n",
    "    start_t, end_t = time_window\n",
    "    now = datetime.datetime.now().time()\n",
    "    if not (start_t <= now <= end_t):\n",
    "        print(f\"Current time {now}. Outside attendance window {start_t} - {end_t}.\")\n",
    "        return\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    attendance = {} \n",
    "    print(\"Attendance running. Press 'q' to stop early.\")\n",
    "\n",
    "    emotion_labels = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        boxes = face_recognition.face_locations(rgb, model='hog')\n",
    "        encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "\n",
    "        for (box, encoding) in zip(boxes, encodings):\n",
    "            probs = clf.predict_proba([encoding])[0]\n",
    "            idx = np.argmax(probs)\n",
    "            name = le.inverse_transform([idx])[0]\n",
    "            confidence = probs[idx]\n",
    "\n",
    "            top_emotion = 'Unknown'\n",
    "            (top, right, bottom, left) = box\n",
    "            face_img = frame[top:bottom, left:right]\n",
    "            if face_img.size != 0 and emotion_model is not None:\n",
    "                gray = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
    "                gray = cv2.resize(gray, (48,48))\n",
    "                inp = gray.astype('float32')/255.0\n",
    "                inp = np.expand_dims(inp, axis=(0,-1))\n",
    "                pred = emotion_model.predict(inp)\n",
    "                eidx = np.argmax(pred)\n",
    "                top_emotion = emotion_labels[eidx] if eidx < len(emotion_labels) else 'Unknown'\n",
    "\n",
    "            ts = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            if name not in attendance:\n",
    "                attendance[name] = {'first_seen': ts, 'last_emotion': top_emotion, 'confidence': float(confidence)}\n",
    "            else:\n",
    "                attendance[name]['last_emotion'] = top_emotion\n",
    "                attendance[name]['confidence'] = float(confidence)\n",
    "\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (0,255,0), 2)\n",
    "            cv2.putText(frame, f\"{name} {confidence:.2f}\", (left, top-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0),2)\n",
    "            cv2.putText(frame, f\"{top_emotion}\", (left, bottom+20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,0,0),2)\n",
    "\n",
    "        cv2.imshow(\"Attendance\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    rows = []\n",
    "    for name, info in attendance.items():\n",
    "        rows.append({\n",
    "            'Name': name,\n",
    "            'Status': 'Present',\n",
    "            'FirstSeen': info['first_seen'],\n",
    "            'LastEmotion': info['last_emotion'],\n",
    "            'Confidence': info.get('confidence', None)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=['Name','Status','FirstSeen','LastEmotion','Confidence'])\n",
    "    all_students = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
    "    for s in all_students:\n",
    "        if s not in df['Name'].values:\n",
    "            df = df._append({'Name': s, 'Status': 'Absent', 'FirstSeen': None, 'LastEmotion': None, 'Confidence': None}, ignore_index=True)\n",
    "\n",
    "    df.to_csv(ATTENDANCE_CSV, index=False)\n",
    "    print(f\"Attendance saved to {ATTENDANCE_CSV}\")\n",
    "\n",
    "recognize_and_mark_attendance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0dc5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
